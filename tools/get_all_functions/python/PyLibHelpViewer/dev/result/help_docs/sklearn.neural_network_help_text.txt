Help on package sklearn.neural_network in sklearn:

NAME
    sklearn.neural_network

DESCRIPTION
    The :mod:`sklearn.neural_network` module includes models based on neural
    networks.

PACKAGE CONTENTS
    _base
    _stochastic_optimizers
    multilayer_perceptron
    rbm
    tests (package)

CLASSES
    sklearn.base.BaseEstimator(builtins.object)
        sklearn.neural_network.rbm.BernoulliRBM(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin)
    sklearn.base.ClassifierMixin(builtins.object)
        sklearn.neural_network.multilayer_perceptron.MLPClassifier(sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron, sklearn.base.ClassifierMixin)
    sklearn.base.RegressorMixin(builtins.object)
        sklearn.neural_network.multilayer_perceptron.MLPRegressor(sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron, sklearn.base.RegressorMixin)
    sklearn.base.TransformerMixin(builtins.object)
        sklearn.neural_network.rbm.BernoulliRBM(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin)
    sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron(abc.NewBase)
        sklearn.neural_network.multilayer_perceptron.MLPClassifier(sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron, sklearn.base.ClassifierMixin)
        sklearn.neural_network.multilayer_perceptron.MLPRegressor(sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron, sklearn.base.RegressorMixin)
    
    class BernoulliRBM(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin)
     |  Bernoulli Restricted Boltzmann Machine (RBM).
     |  
     |  A Restricted Boltzmann Machine with binary visible units and
     |  binary hidden units. Parameters are estimated using Stochastic Maximum
     |  Likelihood (SML), also known as Persistent Contrastive Divergence (PCD)
     |  [2].
     |  
     |  The time complexity of this implementation is ``O(d ** 2)`` assuming
     |  d ~ n_features ~ n_components.
     |  
     |  Read more in the :ref:`User Guide <rbm>`.
     |  
     |  Parameters
     |  ----------
     |  n_components : int, optional
     |      Number of binary hidden units.
     |  
     |  learning_rate : float, optional
     |      The learning rate for weight updates. It is *highly* recommended
     |      to tune this hyper-parameter. Reasonable values are in the
     |      10**[0., -3.] range.
     |  
     |  batch_size : int, optional
     |      Number of examples per minibatch.
     |  
     |  n_iter : int, optional
     |      Number of iterations/sweeps over the training dataset to perform
     |      during training.
     |  
     |  verbose : int, optional
     |      The verbosity level. The default, zero, means silent mode.
     |  
     |  random_state : integer or numpy.RandomState, optional
     |      A random number generator instance to define the state of the
     |      random permutations generator. If an integer is given, it fixes the
     |      seed. Defaults to the global numpy random number generator.
     |  
     |  Attributes
     |  ----------
     |  intercept_hidden_ : array-like, shape (n_components,)
     |      Biases of the hidden units.
     |  
     |  intercept_visible_ : array-like, shape (n_features,)
     |      Biases of the visible units.
     |  
     |  components_ : array-like, shape (n_components, n_features)
     |      Weight matrix, where n_features in the number of
     |      visible units and n_components is the number of hidden units.
     |  
     |  Examples
     |  --------
     |  
     |  >>> import numpy as np
     |  >>> from sklearn.neural_network import BernoulliRBM
     |  >>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
     |  >>> model = BernoulliRBM(n_components=2)
     |  >>> model.fit(X)
     |  BernoulliRBM(batch_size=10, learning_rate=0.1, n_components=2, n_iter=10,
     |         random_state=None, verbose=0)
     |  
     |  References
     |  ----------
     |  
     |  [1] Hinton, G. E., Osindero, S. and Teh, Y. A fast learning algorithm for
     |      deep belief nets. Neural Computation 18, pp 1527-1554.
     |      http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf
     |  
     |  [2] Tieleman, T. Training Restricted Boltzmann Machines using
     |      Approximations to the Likelihood Gradient. International Conference
     |      on Machine Learning (ICML) 2008
     |  
     |  Method resolution order:
     |      BernoulliRBM
     |      sklearn.base.BaseEstimator
     |      sklearn.base.TransformerMixin
     |      builtins.object
     |  
     |  Methods defined here:
     |  
     |  __init__(self, n_components=256, learning_rate=0.1, batch_size=10, n_iter=10, verbose=0, random_state=None)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |  
     |  fit(self, X, y=None)
     |      Fit the model to the data X.
     |      
     |      Parameters
     |      ----------
     |      X : {array-like, sparse matrix} shape (n_samples, n_features)
     |          Training data.
     |      
     |      Returns
     |      -------
     |      self : BernoulliRBM
     |          The fitted model.
     |  
     |  gibbs(self, v)
     |      Perform one Gibbs sampling step.
     |      
     |      Parameters
     |      ----------
     |      v : array-like, shape (n_samples, n_features)
     |          Values of the visible layer to start from.
     |      
     |      Returns
     |      -------
     |      v_new : array-like, shape (n_samples, n_features)
     |          Values of the visible layer after one Gibbs step.
     |  
     |  partial_fit(self, X, y=None)
     |      Fit the model to the data X which should contain a partial
     |      segment of the data.
     |      
     |      Parameters
     |      ----------
     |      X : array-like, shape (n_samples, n_features)
     |          Training data.
     |      
     |      Returns
     |      -------
     |      self : BernoulliRBM
     |          The fitted model.
     |  
     |  score_samples(self, X)
     |      Compute the pseudo-likelihood of X.
     |      
     |      Parameters
     |      ----------
     |      X : {array-like, sparse matrix} shape (n_samples, n_features)
     |          Values of the visible layer. Must be all-boolean (not checked).
     |      
     |      Returns
     |      -------
     |      pseudo_likelihood : array-like, shape (n_samples,)
     |          Value of the pseudo-likelihood (proxy for likelihood).
     |      
     |      Notes
     |      -----
     |      This method is not deterministic: it computes a quantity called the
     |      free energy on X, then on a randomly corrupted version of X, and
     |      returns the log of the logistic function of the difference.
     |  
     |  transform(self, X)
     |      Compute the hidden layer activation probabilities, P(h=1|v=X).
     |      
     |      Parameters
     |      ----------
     |      X : {array-like, sparse matrix} shape (n_samples, n_features)
     |          The data to be transformed.
     |      
     |      Returns
     |      -------
     |      h : array, shape (n_samples, n_components)
     |          Latent representations of the data.
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |  
     |  __getstate__(self)
     |  
     |  __repr__(self)
     |      Return repr(self).
     |  
     |  __setstate__(self, state)
     |  
     |  get_params(self, deep=True)
     |      Get parameters for this estimator.
     |      
     |      Parameters
     |      ----------
     |      deep : boolean, optional
     |          If True, will return the parameters for this estimator and
     |          contained subobjects that are estimators.
     |      
     |      Returns
     |      -------
     |      params : mapping of string to any
     |          Parameter names mapped to their values.
     |  
     |  set_params(self, **params)
     |      Set the parameters of this estimator.
     |      
     |      The method works on simple estimators as well as on nested objects
     |      (such as pipelines). The latter have parameters of the form
     |      ``<component>__<parameter>`` so that it's possible to update each
     |      component of a nested object.
     |      
     |      Returns
     |      -------
     |      self
     |  
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.base.BaseEstimator:
     |  
     |  __dict__
     |      dictionary for instance variables (if defined)
     |  
     |  __weakref__
     |      list of weak references to the object (if defined)
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.TransformerMixin:
     |  
     |  fit_transform(self, X, y=None, **fit_params)
     |      Fit to data, then transform it.
     |      
     |      Fits transformer to X and y with optional parameters fit_params
     |      and returns a transformed version of X.
     |      
     |      Parameters
     |      ----------
     |      X : numpy array of shape [n_samples, n_features]
     |          Training set.
     |      
     |      y : numpy array of shape [n_samples]
     |          Target values.
     |      
     |      Returns
     |      -------
     |      X_new : numpy array of shape [n_samples, n_features_new]
     |          Transformed array.
    
    class MLPClassifier(BaseMultilayerPerceptron, sklearn.base.ClassifierMixin)
     |  Multi-layer Perceptron classifier.
     |  
     |  This model optimizes the log-loss function using LBFGS or stochastic
     |  gradient descent.
     |  
     |  .. versionadded:: 0.18
     |  
     |  Parameters
     |  ----------
     |  hidden_layer_sizes : tuple, length = n_layers - 2, default (100,)
     |      The ith element represents the number of neurons in the ith
     |      hidden layer.
     |  
     |  activation : {'identity', 'logistic', 'tanh', 'relu'}, default 'relu'
     |      Activation function for the hidden layer.
     |  
     |      - 'identity', no-op activation, useful to implement linear bottleneck,
     |        returns f(x) = x
     |  
     |      - 'logistic', the logistic sigmoid function,
     |        returns f(x) = 1 / (1 + exp(-x)).
     |  
     |      - 'tanh', the hyperbolic tan function,
     |        returns f(x) = tanh(x).
     |  
     |      - 'relu', the rectified linear unit function,
     |        returns f(x) = max(0, x)
     |  
     |  solver : {'lbfgs', 'sgd', 'adam'}, default 'adam'
     |      The solver for weight optimization.
     |  
     |      - 'lbfgs' is an optimizer in the family of quasi-Newton methods.
     |  
     |      - 'sgd' refers to stochastic gradient descent.
     |  
     |      - 'adam' refers to a stochastic gradient-based optimizer proposed
     |        by Kingma, Diederik, and Jimmy Ba
     |  
     |      Note: The default solver 'adam' works pretty well on relatively
     |      large datasets (with thousands of training samples or more) in terms of
     |      both training time and validation score.
     |      For small datasets, however, 'lbfgs' can converge faster and perform
     |      better.
     |  
     |  alpha : float, optional, default 0.0001
     |      L2 penalty (regularization term) parameter.
     |  
     |  batch_size : int, optional, default 'auto'
     |      Size of minibatches for stochastic optimizers.
     |      If the solver is 'lbfgs', the classifier will not use minibatch.
     |      When set to "auto", `batch_size=min(200, n_samples)`
     |  
     |  learning_rate : {'constant', 'invscaling', 'adaptive'}, default 'constant'
     |      Learning rate schedule for weight updates.
     |  
     |      - 'constant' is a constant learning rate given by
     |        'learning_rate_init'.
     |  
     |      - 'invscaling' gradually decreases the learning rate ``learning_rate_``
     |        at each time step 't' using an inverse scaling exponent of 'power_t'.
     |        effective_learning_rate = learning_rate_init / pow(t, power_t)
     |  
     |      - 'adaptive' keeps the learning rate constant to
     |        'learning_rate_init' as long as training loss keeps decreasing.
     |        Each time two consecutive epochs fail to decrease training loss by at
     |        least tol, or fail to increase validation score by at least tol if
     |        'early_stopping' is on, the current learning rate is divided by 5.
     |  
     |      Only used when ``solver='sgd'``.
     |  
     |  learning_rate_init : double, optional, default 0.001
     |      The initial learning rate used. It controls the step-size
     |      in updating the weights. Only used when solver='sgd' or 'adam'.
     |  
     |  power_t : double, optional, default 0.5
     |      The exponent for inverse scaling learning rate.
     |      It is used in updating effective learning rate when the learning_rate
     |      is set to 'invscaling'. Only used when solver='sgd'.
     |  
     |  max_iter : int, optional, default 200
     |      Maximum number of iterations. The solver iterates until convergence
     |      (determined by 'tol') or this number of iterations. For stochastic
     |      solvers ('sgd', 'adam'), note that this determines the number of epochs
     |      (how many times each data point will be used), not the number of
     |      gradient steps.
     |  
     |  shuffle : bool, optional, default True
     |      Whether to shuffle samples in each iteration. Only used when
     |      solver='sgd' or 'adam'.
     |  
     |  random_state : int, RandomState instance or None, optional, default None
     |      If int, random_state is the seed used by the random number generator;
     |      If RandomState instance, random_state is the random number generator;
     |      If None, the random number generator is the RandomState instance used
     |      by `np.random`.
     |  
     |  tol : float, optional, default 1e-4
     |      Tolerance for the optimization. When the loss or score is not improving
     |      by at least tol for two consecutive iterations, unless `learning_rate`
     |      is set to 'adaptive', convergence is considered to be reached and
     |      training stops.
     |  
     |  verbose : bool, optional, default False
     |      Whether to print progress messages to stdout.
     |  
     |  warm_start : bool, optional, default False
     |      When set to True, reuse the solution of the previous
     |      call to fit as initialization, otherwise, just erase the
     |      previous solution.
     |  
     |  momentum : float, default 0.9
     |      Momentum for gradient descent update. Should be between 0 and 1. Only
     |      used when solver='sgd'.
     |  
     |  nesterovs_momentum : boolean, default True
     |      Whether to use Nesterov's momentum. Only used when solver='sgd' and
     |      momentum > 0.
     |  
     |  early_stopping : bool, default False
     |      Whether to use early stopping to terminate training when validation
     |      score is not improving. If set to true, it will automatically set
     |      aside 10% of training data as validation and terminate training when
     |      validation score is not improving by at least tol for two consecutive
     |      epochs.
     |      Only effective when solver='sgd' or 'adam'
     |  
     |  validation_fraction : float, optional, default 0.1
     |      The proportion of training data to set aside as validation set for
     |      early stopping. Must be between 0 and 1.
     |      Only used if early_stopping is True
     |  
     |  beta_1 : float, optional, default 0.9
     |      Exponential decay rate for estimates of first moment vector in adam,
     |      should be in [0, 1). Only used when solver='adam'
     |  
     |  beta_2 : float, optional, default 0.999
     |      Exponential decay rate for estimates of second moment vector in adam,
     |      should be in [0, 1). Only used when solver='adam'
     |  
     |  epsilon : float, optional, default 1e-8
     |      Value for numerical stability in adam. Only used when solver='adam'
     |  
     |  Attributes
     |  ----------
     |  classes_ : array or list of array of shape (n_classes,)
     |      Class labels for each output.
     |  
     |  loss_ : float
     |      The current loss computed with the loss function.
     |  
     |  coefs_ : list, length n_layers - 1
     |      The ith element in the list represents the weight matrix corresponding
     |      to layer i.
     |  
     |  intercepts_ : list, length n_layers - 1
     |      The ith element in the list represents the bias vector corresponding to
     |      layer i + 1.
     |  
     |  n_iter_ : int,
     |      The number of iterations the solver has ran.
     |  
     |  n_layers_ : int
     |      Number of layers.
     |  
     |  n_outputs_ : int
     |      Number of outputs.
     |  
     |  out_activation_ : string
     |      Name of the output activation function.
     |  
     |  Notes
     |  -----
     |  MLPClassifier trains iteratively since at each time step
     |  the partial derivatives of the loss function with respect to the model
     |  parameters are computed to update the parameters.
     |  
     |  It can also have a regularization term added to the loss function
     |  that shrinks model parameters to prevent overfitting.
     |  
     |  This implementation works with data represented as dense numpy arrays or
     |  sparse scipy arrays of floating point values.
     |  
     |  References
     |  ----------
     |  Hinton, Geoffrey E.
     |      "Connectionist learning procedures." Artificial intelligence 40.1
     |      (1989): 185-234.
     |  
     |  Glorot, Xavier, and Yoshua Bengio. "Understanding the difficulty of
     |      training deep feedforward neural networks." International Conference
     |      on Artificial Intelligence and Statistics. 2010.
     |  
     |  He, Kaiming, et al. "Delving deep into rectifiers: Surpassing human-level
     |      performance on imagenet classification." arXiv preprint
     |      arXiv:1502.01852 (2015).
     |  
     |  Kingma, Diederik, and Jimmy Ba. "Adam: A method for stochastic
     |      optimization." arXiv preprint arXiv:1412.6980 (2014).
     |  
     |  Method resolution order:
     |      MLPClassifier
     |      BaseMultilayerPerceptron
     |      abc.NewBase
     |      sklearn.base.BaseEstimator
     |      sklearn.base.ClassifierMixin
     |      builtins.object
     |  
     |  Methods defined here:
     |  
     |  __init__(self, hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |  
     |  fit(self, X, y)
     |      Fit the model to data matrix X and target(s) y.
     |      
     |      Parameters
     |      ----------
     |      X : array-like or sparse matrix, shape (n_samples, n_features)
     |          The input data.
     |      
     |      y : array-like, shape (n_samples,) or (n_samples, n_outputs)
     |          The target values (class labels in classification, real numbers in
     |          regression).
     |      
     |      Returns
     |      -------
     |      self : returns a trained MLP model.
     |  
     |  predict(self, X)
     |      Predict using the multi-layer perceptron classifier
     |      
     |      Parameters
     |      ----------
     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)
     |          The input data.
     |      
     |      Returns
     |      -------
     |      y : array-like, shape (n_samples,) or (n_samples, n_classes)
     |          The predicted classes.
     |  
     |  predict_log_proba(self, X)
     |      Return the log of probability estimates.
     |      
     |      Parameters
     |      ----------
     |      X : array-like, shape (n_samples, n_features)
     |          The input data.
     |      
     |      Returns
     |      -------
     |      log_y_prob : array-like, shape (n_samples, n_classes)
     |          The predicted log-probability of the sample for each class
     |          in the model, where classes are ordered as they are in
     |          `self.classes_`. Equivalent to log(predict_proba(X))
     |  
     |  predict_proba(self, X)
     |      Probability estimates.
     |      
     |      Parameters
     |      ----------
     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)
     |          The input data.
     |      
     |      Returns
     |      -------
     |      y_prob : array-like, shape (n_samples, n_classes)
     |          The predicted probability of the sample for each class in the
     |          model, where classes are ordered as they are in `self.classes_`.
     |  
     |  ----------------------------------------------------------------------
     |  Data descriptors defined here:
     |  
     |  partial_fit
     |      Fit the model to data matrix X and target y.
     |      
     |      Parameters
     |      ----------
     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)
     |          The input data.
     |      
     |      y : array-like, shape (n_samples,)
     |          The target values.
     |      
     |      classes : array, shape (n_classes)
     |          Classes across all calls to partial_fit.
     |          Can be obtained via `np.unique(y_all)`, where y_all is the
     |          target vector of the entire dataset.
     |          This argument is required for the first call to partial_fit
     |          and can be omitted in the subsequent calls.
     |          Note that y doesn't need to contain all labels in `classes`.
     |      
     |      Returns
     |      -------
     |      self : returns a trained MLP model.
     |  
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |  
     |  __abstractmethods__ = frozenset()
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |  
     |  __getstate__(self)
     |  
     |  __repr__(self)
     |      Return repr(self).
     |  
     |  __setstate__(self, state)
     |  
     |  get_params(self, deep=True)
     |      Get parameters for this estimator.
     |      
     |      Parameters
     |      ----------
     |      deep : boolean, optional
     |          If True, will return the parameters for this estimator and
     |          contained subobjects that are estimators.
     |      
     |      Returns
     |      -------
     |      params : mapping of string to any
     |          Parameter names mapped to their values.
     |  
     |  set_params(self, **params)
     |      Set the parameters of this estimator.
     |      
     |      The method works on simple estimators as well as on nested objects
     |      (such as pipelines). The latter have parameters of the form
     |      ``<component>__<parameter>`` so that it's possible to update each
     |      component of a nested object.
     |      
     |      Returns
     |      -------
     |      self
     |  
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.base.BaseEstimator:
     |  
     |  __dict__
     |      dictionary for instance variables (if defined)
     |  
     |  __weakref__
     |      list of weak references to the object (if defined)
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.ClassifierMixin:
     |  
     |  score(self, X, y, sample_weight=None)
     |      Returns the mean accuracy on the given test data and labels.
     |      
     |      In multi-label classification, this is the subset accuracy
     |      which is a harsh metric since you require for each sample that
     |      each label set be correctly predicted.
     |      
     |      Parameters
     |      ----------
     |      X : array-like, shape = (n_samples, n_features)
     |          Test samples.
     |      
     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)
     |          True labels for X.
     |      
     |      sample_weight : array-like, shape = [n_samples], optional
     |          Sample weights.
     |      
     |      Returns
     |      -------
     |      score : float
     |          Mean accuracy of self.predict(X) wrt. y.
    
    class MLPRegressor(BaseMultilayerPerceptron, sklearn.base.RegressorMixin)
     |  Multi-layer Perceptron regressor.
     |  
     |  This model optimizes the squared-loss using LBFGS or stochastic gradient
     |  descent.
     |  
     |  .. versionadded:: 0.18
     |  
     |  Parameters
     |  ----------
     |  hidden_layer_sizes : tuple, length = n_layers - 2, default (100,)
     |      The ith element represents the number of neurons in the ith
     |      hidden layer.
     |  
     |  activation : {'identity', 'logistic', 'tanh', 'relu'}, default 'relu'
     |      Activation function for the hidden layer.
     |  
     |      - 'identity', no-op activation, useful to implement linear bottleneck,
     |        returns f(x) = x
     |  
     |      - 'logistic', the logistic sigmoid function,
     |        returns f(x) = 1 / (1 + exp(-x)).
     |  
     |      - 'tanh', the hyperbolic tan function,
     |        returns f(x) = tanh(x).
     |  
     |      - 'relu', the rectified linear unit function,
     |        returns f(x) = max(0, x)
     |  
     |  solver : {'lbfgs', 'sgd', 'adam'}, default 'adam'
     |      The solver for weight optimization.
     |  
     |      - 'lbfgs' is an optimizer in the family of quasi-Newton methods.
     |  
     |      - 'sgd' refers to stochastic gradient descent.
     |  
     |      - 'adam' refers to a stochastic gradient-based optimizer proposed by
     |        Kingma, Diederik, and Jimmy Ba
     |  
     |      Note: The default solver 'adam' works pretty well on relatively
     |      large datasets (with thousands of training samples or more) in terms of
     |      both training time and validation score.
     |      For small datasets, however, 'lbfgs' can converge faster and perform
     |      better.
     |  
     |  alpha : float, optional, default 0.0001
     |      L2 penalty (regularization term) parameter.
     |  
     |  batch_size : int, optional, default 'auto'
     |      Size of minibatches for stochastic optimizers.
     |      If the solver is 'lbfgs', the classifier will not use minibatch.
     |      When set to "auto", `batch_size=min(200, n_samples)`
     |  
     |  learning_rate : {'constant', 'invscaling', 'adaptive'}, default 'constant'
     |      Learning rate schedule for weight updates.
     |  
     |      - 'constant' is a constant learning rate given by
     |        'learning_rate_init'.
     |  
     |      - 'invscaling' gradually decreases the learning rate ``learning_rate_``
     |        at each time step 't' using an inverse scaling exponent of 'power_t'.
     |        effective_learning_rate = learning_rate_init / pow(t, power_t)
     |  
     |      - 'adaptive' keeps the learning rate constant to
     |        'learning_rate_init' as long as training loss keeps decreasing.
     |        Each time two consecutive epochs fail to decrease training loss by at
     |        least tol, or fail to increase validation score by at least tol if
     |        'early_stopping' is on, the current learning rate is divided by 5.
     |  
     |      Only used when solver='sgd'.
     |  
     |  learning_rate_init : double, optional, default 0.001
     |      The initial learning rate used. It controls the step-size
     |      in updating the weights. Only used when solver='sgd' or 'adam'.
     |  
     |  power_t : double, optional, default 0.5
     |      The exponent for inverse scaling learning rate.
     |      It is used in updating effective learning rate when the learning_rate
     |      is set to 'invscaling'. Only used when solver='sgd'.
     |  
     |  max_iter : int, optional, default 200
     |      Maximum number of iterations. The solver iterates until convergence
     |      (determined by 'tol') or this number of iterations. For stochastic
     |      solvers ('sgd', 'adam'), note that this determines the number of epochs
     |      (how many times each data point will be used), not the number of
     |      gradient steps.
     |  
     |  shuffle : bool, optional, default True
     |      Whether to shuffle samples in each iteration. Only used when
     |      solver='sgd' or 'adam'.
     |  
     |  random_state : int, RandomState instance or None, optional, default None
     |      If int, random_state is the seed used by the random number generator;
     |      If RandomState instance, random_state is the random number generator;
     |      If None, the random number generator is the RandomState instance used
     |      by `np.random`.
     |  
     |  tol : float, optional, default 1e-4
     |      Tolerance for the optimization. When the loss or score is not improving
     |      by at least tol for two consecutive iterations, unless `learning_rate`
     |      is set to 'adaptive', convergence is considered to be reached and
     |      training stops.
     |  
     |  verbose : bool, optional, default False
     |      Whether to print progress messages to stdout.
     |  
     |  warm_start : bool, optional, default False
     |      When set to True, reuse the solution of the previous
     |      call to fit as initialization, otherwise, just erase the
     |      previous solution.
     |  
     |  momentum : float, default 0.9
     |      Momentum for gradient descent update.  Should be between 0 and 1. Only
     |      used when solver='sgd'.
     |  
     |  nesterovs_momentum : boolean, default True
     |      Whether to use Nesterov's momentum. Only used when solver='sgd' and
     |      momentum > 0.
     |  
     |  early_stopping : bool, default False
     |      Whether to use early stopping to terminate training when validation
     |      score is not improving. If set to true, it will automatically set
     |      aside 10% of training data as validation and terminate training when
     |      validation score is not improving by at least tol for two consecutive
     |      epochs.
     |      Only effective when solver='sgd' or 'adam'
     |  
     |  validation_fraction : float, optional, default 0.1
     |      The proportion of training data to set aside as validation set for
     |      early stopping. Must be between 0 and 1.
     |      Only used if early_stopping is True
     |  
     |  beta_1 : float, optional, default 0.9
     |      Exponential decay rate for estimates of first moment vector in adam,
     |      should be in [0, 1). Only used when solver='adam'
     |  
     |  beta_2 : float, optional, default 0.999
     |      Exponential decay rate for estimates of second moment vector in adam,
     |      should be in [0, 1). Only used when solver='adam'
     |  
     |  epsilon : float, optional, default 1e-8
     |      Value for numerical stability in adam. Only used when solver='adam'
     |  
     |  Attributes
     |  ----------
     |  loss_ : float
     |      The current loss computed with the loss function.
     |  
     |  coefs_ : list, length n_layers - 1
     |      The ith element in the list represents the weight matrix corresponding
     |      to layer i.
     |  
     |  intercepts_ : list, length n_layers - 1
     |      The ith element in the list represents the bias vector corresponding to
     |      layer i + 1.
     |  
     |  n_iter_ : int,
     |      The number of iterations the solver has ran.
     |  
     |  n_layers_ : int
     |      Number of layers.
     |  
     |  n_outputs_ : int
     |      Number of outputs.
     |  
     |  out_activation_ : string
     |      Name of the output activation function.
     |  
     |  Notes
     |  -----
     |  MLPRegressor trains iteratively since at each time step
     |  the partial derivatives of the loss function with respect to the model
     |  parameters are computed to update the parameters.
     |  
     |  It can also have a regularization term added to the loss function
     |  that shrinks model parameters to prevent overfitting.
     |  
     |  This implementation works with data represented as dense and sparse numpy
     |  arrays of floating point values.
     |  
     |  References
     |  ----------
     |  Hinton, Geoffrey E.
     |      "Connectionist learning procedures." Artificial intelligence 40.1
     |      (1989): 185-234.
     |  
     |  Glorot, Xavier, and Yoshua Bengio. "Understanding the difficulty of
     |      training deep feedforward neural networks." International Conference
     |      on Artificial Intelligence and Statistics. 2010.
     |  
     |  He, Kaiming, et al. "Delving deep into rectifiers: Surpassing human-level
     |      performance on imagenet classification." arXiv preprint
     |      arXiv:1502.01852 (2015).
     |  
     |  Kingma, Diederik, and Jimmy Ba. "Adam: A method for stochastic
     |      optimization." arXiv preprint arXiv:1412.6980 (2014).
     |  
     |  Method resolution order:
     |      MLPRegressor
     |      BaseMultilayerPerceptron
     |      abc.NewBase
     |      sklearn.base.BaseEstimator
     |      sklearn.base.RegressorMixin
     |      builtins.object
     |  
     |  Methods defined here:
     |  
     |  __init__(self, hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |  
     |  predict(self, X)
     |      Predict using the multi-layer perceptron model.
     |      
     |      Parameters
     |      ----------
     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)
     |          The input data.
     |      
     |      Returns
     |      -------
     |      y : array-like, shape (n_samples, n_outputs)
     |          The predicted values.
     |  
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |  
     |  __abstractmethods__ = frozenset()
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from BaseMultilayerPerceptron:
     |  
     |  fit(self, X, y)
     |      Fit the model to data matrix X and target(s) y.
     |      
     |      Parameters
     |      ----------
     |      X : array-like or sparse matrix, shape (n_samples, n_features)
     |          The input data.
     |      
     |      y : array-like, shape (n_samples,) or (n_samples, n_outputs)
     |          The target values (class labels in classification, real numbers in
     |          regression).
     |      
     |      Returns
     |      -------
     |      self : returns a trained MLP model.
     |  
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from BaseMultilayerPerceptron:
     |  
     |  partial_fit
     |      Fit the model to data matrix X and target y.
     |      
     |      Parameters
     |      ----------
     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)
     |          The input data.
     |      
     |      y : array-like, shape (n_samples,)
     |          The target values.
     |      
     |      Returns
     |      -------
     |      self : returns a trained MLP model.
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |  
     |  __getstate__(self)
     |  
     |  __repr__(self)
     |      Return repr(self).
     |  
     |  __setstate__(self, state)
     |  
     |  get_params(self, deep=True)
     |      Get parameters for this estimator.
     |      
     |      Parameters
     |      ----------
     |      deep : boolean, optional
     |          If True, will return the parameters for this estimator and
     |          contained subobjects that are estimators.
     |      
     |      Returns
     |      -------
     |      params : mapping of string to any
     |          Parameter names mapped to their values.
     |  
     |  set_params(self, **params)
     |      Set the parameters of this estimator.
     |      
     |      The method works on simple estimators as well as on nested objects
     |      (such as pipelines). The latter have parameters of the form
     |      ``<component>__<parameter>`` so that it's possible to update each
     |      component of a nested object.
     |      
     |      Returns
     |      -------
     |      self
     |  
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.base.BaseEstimator:
     |  
     |  __dict__
     |      dictionary for instance variables (if defined)
     |  
     |  __weakref__
     |      list of weak references to the object (if defined)
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.RegressorMixin:
     |  
     |  score(self, X, y, sample_weight=None)
     |      Returns the coefficient of determination R^2 of the prediction.
     |      
     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual
     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total
     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().
     |      The best possible score is 1.0 and it can be negative (because the
     |      model can be arbitrarily worse). A constant model that always
     |      predicts the expected value of y, disregarding the input features,
     |      would get a R^2 score of 0.0.
     |      
     |      Parameters
     |      ----------
     |      X : array-like, shape = (n_samples, n_features)
     |          Test samples.
     |      
     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)
     |          True values for X.
     |      
     |      sample_weight : array-like, shape = [n_samples], optional
     |          Sample weights.
     |      
     |      Returns
     |      -------
     |      score : float
     |          R^2 of self.predict(X) wrt. y.

DATA
    __all__ = ['BernoulliRBM', 'MLPClassifier', 'MLPRegressor']

FILE
    /Users/jack/miniconda2/envs/py37/lib/python3.6/site-packages/sklearn/neural_network/__init__.py


